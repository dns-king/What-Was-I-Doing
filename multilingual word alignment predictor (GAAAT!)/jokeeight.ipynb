{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZIjS6fH17dhsCMB+Txe73"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Install Packs!"],"metadata":{"id":"Mn3C_XRgDkg-"}},{"cell_type":"code","source":["!pip install --upgrade pip\n","!pip install transformers sentence-transformers torch torchvision torchaudio networkx scikit-learn numpy pandas tqdm\n","!pip install torch-geometric"],"metadata":{"id":"Jkj4gO6XNAa_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imports!!"],"metadata":{"id":"bB4WbQ_DDnpc"}},{"cell_type":"code","source":["import random\n","import itertools\n","import math\n","from collections import defaultdict\n","from typing import List, Tuple, Dict\n","\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.preprocessing import normalize\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","\n","from tqdm.auto import tqdm\n","\n","# PyG\n","import torch_geometric\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GATConv\n","\n","# Transformers (mBERT)\n","from transformers import AutoTokenizer, AutoModel\n","# better than me... blood orange is incredible!\n"],"metadata":{"id":"LWZrQUurNEwf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Multiparallel Corpus (synthetic)"],"metadata":{"id":"Ca2oomTTFGOY"}},{"cell_type":"code","source":["# Small synthetic vocabulary with translations for English -> other languages.\n","synthetic_vocab = [\n","    (\"cat\", \"Katze\", \"gato\", \"γάτα\"),\n","    (\"dog\", \"Hund\", \"perro\", \"σκύλος\"),\n","    (\"house\", \"Haus\", \"casa\", \"σπίτι\"),\n","    (\"blue\", \"blau\", \"azul\", \"μπλε\"),\n","    (\"eat\", \"essen\", \"comer\", \"τρώω\"),\n","    (\"sleep\", \"schlafen\", \"dormir\", \"κοιμάμαι\"),\n","    (\"small\", \"klein\", \"pequeño\", \"μικρό\"),\n","    (\"big\", \"groß\", \"grande\", \"μεγάλο\"),\n","    (\"man\", \"Mann\", \"hombre\", \"άνδρας\"),\n","    (\"woman\", \"Frau\", \"mujer\", \"γυναίκα\"),\n","    (\"see\", \"sehen\", \"ver\", \"βλέπω\"),\n","    (\"love\", \"lieben\", \"amar\", \"αγαπώ\"),\n","]\n","\n","# synthetic sentences by sampling small templates\n","templates = [\n","    (\"The {a} is {adj}\", \"{a} ist {adj}\", \"El {a} es {adj}\", \"Η {a} είναι {adj}\"),\n","    (\"A {adj} {a}\", \"Ein {adj} {a}\", \"Un {adj} {a}\", \"Ένα {adj} {a}\"),\n","    (\"I {v} the {a}\", \"Ich {v} die {a}\", \"Yo {v} al {a}\", \"Εγώ {v} το {a}\"),\n","    (\"The {a} and the {b}\", \"Die {a} und die {b}\", \"El {a} y el {b}\", \"Η {a} και η {b}\"),\n","]\n","\n","lang_index = {\"en\": 0, \"de\": 1, \"es\": 2, \"el\": 3}\n","\n","def build_sentence_pair(template):\n","    # chose two distinct nouns and one adjective and one verb\n","    nouns = random.sample(synthetic_vocab, 2)\n","    adj = random.choice([w for w in synthetic_vocab if w[0] in (\"small\",\"big\",\"blue\")])\n","    verb = random.choice([w for w in synthetic_vocab if w[0] in (\"eat\",\"sleep\",\"see\",\"love\")])\n","    # Fill template\n","    en_t, de_t, es_t, el_t = template\n","    a_en = nouns[0][0]; b_en = nouns[1][0]; adj_en = adj[0]; v_en = verb[0]\n","    a_de = nouns[0][1]; b_de = nouns[1][1]; adj_de = adj[1]; v_de = verb[1]\n","    a_es = nouns[0][2]; b_es = nouns[1][2]; adj_es = adj[2]; v_es = verb[2]\n","    a_el = nouns[0][3]; b_el = nouns[1][3]; adj_el = adj[3]; v_el = verb[3]\n","    # Simple formatting\n","    en = en_t.format(a=a_en, b=b_en, adj=adj_en, v=v_en)\n","    de = de_t.format(a=a_de, b=b_de, adj=adj_de, v=v_de)\n","    es = es_t.format(a=a_es, b=b_es, adj=adj_es, v=v_es)\n","    el = el_t.format(a=a_el, b=b_el, adj=adj_el, v=v_el)\n","    return {\"en\": en, \"de\": de, \"es\": es, \"el\": el}\n","\n","# Build a dataset of sentence quadruples\n","random.seed(42)\n","num_sentences = 200  # size; increase if you want\n","corpus = []\n","for _ in range(num_sentences):\n","    templ = random.choice(templates)\n","    corpus.append(build_sentence_pair(templ))\n","\n","# Quick peek\n","for i in range(5):\n","    print(corpus[i])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jaCaBWTGNWaz","executionInfo":{"status":"ok","timestamp":1763993737791,"user_tz":-120,"elapsed":20,"user":{"displayName":"MANQOBA Nkosi","userId":"08876572203614423140"}},"outputId":"5164712d-7d63-415a-ed4f-0c0684343258"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'en': 'The cat is blue', 'de': 'Katze ist blau', 'es': 'El gato es azul', 'el': 'Η γάτα είναι μπλε'}\n","{'en': 'A big love', 'de': 'Ein groß lieben', 'es': 'Un grande amar', 'el': 'Ένα μεγάλο αγαπώ'}\n","{'en': 'The cat and the love', 'de': 'Die Katze und die lieben', 'es': 'El gato y el amar', 'el': 'Η γάτα και η αγαπώ'}\n","{'en': 'A blue man', 'de': 'Ein blau Mann', 'es': 'Un azul hombre', 'el': 'Ένα μπλε άνδρας'}\n","{'en': 'The blue and the big', 'de': 'Die blau und die groß', 'es': 'El azul y el grande', 'el': 'Η μπλε και η μεγάλο'}\n"]}]},{"cell_type":"markdown","source":["Tokenize... token-level objects(nodes)"],"metadata":{"id":"YRbzeReZFQr4"}},{"cell_type":"code","source":["# Tokenize simply by whitespace\n","# EMBEDDINGS; mBERT tokenization and map word -> averaged subword embeddings.\n","\n","def whitespace_tokenize(s: str):\n","    return s.strip().split()\n","\n","# ode structure: each token gets a unique node id across the whole corpus + meta\n","nodes = []\n","node_id = 0\n","sent_token_index = []  # per sentence: dict mapping (lang -> list of node ids)\n","for s_idx, sent in enumerate(corpus):\n","    mapping = {}\n","    for lang in [\"en\",\"de\",\"es\",\"el\"]:\n","        toks = whitespace_tokenize(sent[lang])\n","        ids = []\n","        for pos, tok in enumerate(toks):\n","            nodes.append({\n","                \"node_id\": node_id,\n","                \"sent_idx\": s_idx,\n","                \"lang\": lang,\n","                \"token\": tok,\n","                \"pos\": pos,\n","                \"sent_len\": len(toks)\n","            })\n","            ids.append(node_id)\n","            node_id += 1\n","        mapping[lang] = ids\n","    sent_token_index.append(mapping)\n","\n","num_nodes = len(nodes)\n","print(\"Total nodes (tokens):\", num_nodes)\n"],"metadata":{"id":"29s1o-nZOFfY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load multilingual tokenizer/model (mBERT) and compute token embeddings"],"metadata":{"id":"yf3-_AE7FuWd"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","model_name = \"bert-base-multilingual-cased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","bert = AutoModel.from_pretrained(model_name).to(device)\n","bert.eval()\n","\n","# helper: compute embedding for a word (average of subword token embeddings)\n","# cache results for identical surface forms\n","from functools import lru_cache\n","\n","@lru_cache(maxsize=10000)\n","def word_embedding(word: str) -> np.ndarray:\n","    # use tokenizer.encode_plus to get input ids and attention mask\n","    with torch.no_grad():\n","        encoded = tokenizer(word, return_tensors=\"pt\", add_special_tokens=True)\n","        input_ids = encoded[\"input_ids\"].to(device)\n","        attention_mask = encoded[\"attention_mask\"].to(device)\n","        outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden = outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden)\n","        # skip [CLS] and [SEP], average the rest\n","        if last_hidden.shape[0] <= 2:\n","            vec = last_hidden.mean(dim=0).cpu().numpy()\n","        else:\n","            vec = last_hidden[1:-1].mean(dim=0).cpu().numpy()\n","    return vec\n","\n","# Compute embeddings for all nodes\n","emb_dim = bert.config.hidden_size\n","X_emb = np.zeros((num_nodes, emb_dim), dtype=np.float32)\n","for nd in tqdm(nodes, desc=\"embeddings\"):\n","    X_emb[nd[\"node_id\"], :] = word_embedding(nd[\"token\"])\n","# normalize embeddings for cosine similarity convenience\n","X_emb = normalize(X_emb, axis=1)\n"],"metadata":{"id":"R9STKzD_OIiC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initial bilingual alignments via embedding similarity (per sentence)"],"metadata":{"id":"-ZHNytbxGEXb"}},{"cell_type":"code","source":["# For each sentence, compute bilingual alignment edges between language pairs using cosine similarity\n","# Threshold-based and 1-to-1 greedy assignment\n","def align_pair(ids_a: List[int], ids_b: List[int], threshold=0.6):\n","    # compute cosine matrix between embeddings\n","    ma = X_emb[ids_a]  # (na, d)\n","    mb = X_emb[ids_b]  # (nb, d)\n","    sim = cosine_similarity(ma, mb)  # (na, nb)\n","    edges = []\n","    # greedy: for eachh a, pick best b if above threshold and not taken; and also allow symmetric bests\n","    taken_b = set()\n","    pairs = []\n","    # flattened sort by sim descending\n","    idx_pairs = [(i,j,sim[i,j]) for i in range(sim.shape[0]) for j in range(sim.shape[1])]\n","    idx_pairs.sort(key=lambda x: x[2], reverse=True)\n","    for i,j,s in idx_pairs:\n","        if s < threshold:\n","            break\n","        if j in taken_b:\n","            continue\n","        taken_b.add(j)\n","        pairs.append((ids_a[i], ids_b[j], float(s)))\n","\n","    return pairs\n","\n","# build initial edge set (undirected)\n","initial_alignment_edges = set()\n","for s_idx, mapping in enumerate(sent_token_index):\n","    # align all language pairs (en-de, en-es, en-el, de-es, de-el, es-el)\n","    langs = [\"en\",\"de\",\"es\",\"el\"]\n","    for la, lb in itertools.combinations(langs, 2):\n","        ids_a = mapping[la]\n","        ids_b = mapping[lb]\n","        pairs = align_pair(ids_a, ids_b, threshold=0.62)\n","        for u,v,score in pairs:\n","            # store as sorted tuple\n","            if u == v:\n","                continue\n","            edge = tuple(sorted((u,v)))\n","            initial_alignment_edges.add(edge)\n","\n","print(\"Initial bilingual alignment edges:\", len(initial_alignment_edges))\n","# show some\n","list(initial_alignment_edges)[:10]\n"],"metadata":{"id":"2RbecvMFOL5F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["multiparallel graph (NetworkX), add intra-sentence adjacency and initial edges"],"metadata":{"id":"V9HZQRPYG4YP"}},{"cell_type":"code","source":["G = nx.Graph()\n","for nd in nodes:\n","    G.add_node(nd[\"node_id\"], **nd)\n","\n","# intra-sentence adjacency edges (connect adjacent tokens within same sentence & language)\n","for s_idx, mapping in enumerate(sent_token_index):\n","    for lang, ids in mapping.items():\n","        for i in range(len(ids)-1):\n","            G.add_edge(ids[i], ids[i+1], label=\"intra\")\n","\n","# initial bilingual alignment edges (label them)\n","for (u,v) in initial_alignment_edges:\n","    G.add_edge(u, v, label=\"bilingual_init\")\n","\n","print(\"Graph summary:\", G)\n","G = nx.Graph()\n","for nd in nodes:\n","    G.add_node(nd[\"node_id\"], **nd)\n","\n","# intra-sentence adjacency edges (connect adjacent tokens within same sentence & language)\n","for s_idx, mapping in enumerate(sent_token_index):\n","    for lang, ids in mapping.items():\n","        for i in range(len(ids)-1):\n","            G.add_edge(ids[i], ids[i+1], label=\"intra\")\n","\n","# add initial bilingual alignment edges (label them)\n","for (u,v) in initial_alignment_edges:\n","    G.add_edge(u, v, label=\"bilingual_init\")\n","\n","print(\"Graph summary:\", G)"],"metadata":{"id":"jWdChPZXOYFS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute graph metrics (degree, pagerank) and community detection"],"metadata":{"id":"uEOBAQo1HHjS"}},{"cell_type":"code","source":["# Degree\n","deg_dict = dict(G.degree())\n","nx.set_node_attributes(G, deg_dict, \"degree\")\n","\n","# PageRank\n","pr = nx.pagerank(G)\n","nx.set_node_attributes(G, pr, \"pagerank\")\n","\n","# Greedy modularity communities (returns sets)\n","communities = list(nx.algorithms.community.greedy_modularity_communities(G))\n","# Map node -> community id (int)\n","node2comm = {}\n","for cid, comm in enumerate(communities):\n","    for n in comm:\n","        node2comm[n] = cid\n","nx.set_node_attributes(G, node2comm, \"community\")\n","\n","print(\"Number of communities:\", len(communities))\n"],"metadata":{"id":"miCF8j2xOcAb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare node feature matrix (concatenate embedding + positional + degree + pagerank + community one-hot)"],"metadata":{"id":"gr78vMLtHMRy"}},{"cell_type":"code","source":["# base embedding fro earlier X_emb\n","# positional: pos / sent_len normalized scalar\n","pos_feat = np.zeros((num_nodes, 1), dtype=np.float32)\n","degree_feat = np.zeros((num_nodes, 1), dtype=np.float32)\n","pagerank_feat = np.zeros((num_nodes, 1), dtype=np.float32)\n","for nd in nodes:\n","    nid = nd[\"node_id\"]\n","    pos_feat[nid,0] = nd[\"pos\"] / max(1, nd[\"sent_len\"] - 1)\n","    degree_feat[nid,0] = deg_dict.get(nid,0)\n","    pagerank_feat[nid,0] = pr.get(nid,0)\n","\n","# community one-hot (small number of communities)\n","num_comms = len(communities)\n","comm_feat = np.zeros((num_nodes, num_comms), dtype=np.float32)\n","for nid, cid in node2comm.items():\n","    comm_feat[nid, cid] = 1.0\n","\n","# normalize degree/pagerank\n","degree_feat = (degree_feat - degree_feat.mean()) / (degree_feat.std() + 1e-8)\n","pagerank_feat = (pagerank_feat - pagerank_feat.mean()) / (pagerank_feat.std() + 1e-8)\n","\n","X = np.concatenate([X_emb, pos_feat, degree_feat, pagerank_feat, comm_feat], axis=1)\n","print(\"Node feature shape:\", X.shape)\n"],"metadata":{"id":"Bzy-AIpvOst_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert to PyG Data (edges + features)"],"metadata":{"id":"RfRkzEucHYvn"}},{"cell_type":"code","source":["# Build edge_index from graph (use undirected symmetric edges)\n","edge_list = list(G.edges())\n","edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()  # shape (2, E)\n","\n","if edge_index.shape[1] == 0:\n","    raise RuntimeError(\"No edges in graph.\")\n","edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)  # add reverse direction\n","\n","x = torch.tensor(X, dtype=torch.float)\n","\n","data = Data(x=x, edge_index=edge_index)\n","print(data)\n"],"metadata":{"id":"a0JV2EZMOwdd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Link prediction dataset (positive and negative samples)"],"metadata":{"id":"hN_EyhJyHnMl"}},{"cell_type":"code","source":["# Positive edges: the bilingual_init edges (need the GNN to learn to predict them).\n","# some of these will be present in graph; use as positives.\n","pos_edges = [tuple(e) for e in initial_alignment_edges]\n","\n","# Negative sampling: sample same number of random node pairs that are NOT in graph and not positives\n","all_pairs_set = set(tuple(sorted((u,v))) for u in range(num_nodes) for v in range(u+1, num_nodes))\n","forbidden = set(initial_alignment_edges) | set(tuple(sorted(e)) for e in G.edges())\n","available = list(all_pairs_set - forbidden)\n","neg_samples = random.sample(available, min(len(pos_edges), len(available)))\n","\n","# Build training tensors (train in-batch)\n","def pairs_to_tensor(pairs):\n","    u = torch.tensor([p[0] for p in pairs], dtype=torch.long)\n","    v = torch.tensor([p[1] for p in pairs], dtype=torch.long)\n","    return u, v\n","\n","pos_u, pos_v = pairs_to_tensor(pos_edges)\n","neg_u, neg_v = pairs_to_tensor(neg_samples)\n","\n","print(\"Positive samples:\", pos_u.size(0), \"Negative samples:\", neg_u.size(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDrKPiipOzba","executionInfo":{"status":"ok","timestamp":1763993953083,"user_tz":-120,"elapsed":12861,"user":{"displayName":"MANQOBA Nkosi","userId":"08876572203614423140"}},"outputId":"54bac7ce-d999-4bb7-e598-76296f764672"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive samples: 1757 Negative samples: 1757\n"]}]},{"cell_type":"markdown","source":["GAT encoder + dot-product decoder for link prediction..."],"metadata":{"id":"1A0TJRxdIGR2"}},{"cell_type":"code","source":["class GATEncoder(nn.Module):\n","    def __init__(self, in_dim, hidden_dim=256, n_heads=4, n_layers=2, dropout=0.2):\n","        super().__init__()\n","        self.layers = nn.ModuleList()\n","        self.layers.append(GATConv(in_dim, hidden_dim // n_heads, heads=n_heads, dropout=dropout))\n","        for _ in range(n_layers-1):\n","            self.layers.append(GATConv(hidden_dim, hidden_dim // n_heads, heads=n_heads, dropout=dropout))\n","        self.dropout = dropout\n","        self.out_dim = hidden_dim\n","\n","    def forward(self, x, edge_index):\n","        for i,layer in enumerate(self.layers):\n","            x = layer(x, edge_index)\n","            x = F.elu(x)\n","            x = F.dropout(x, p=self.dropout, training=self.training)\n","        return x  # node embeddings\n","\n","class LinkPredictor(nn.Module):\n","    def __init__(self, in_dim):\n","        super().__init__()\n","        # use dot product; optionally could add MLP\n","        self.lin = nn.Linear(in_dim*2, in_dim)\n","        self.out = nn.Linear(in_dim, 1)\n","\n","    def forward(self, hu, hv):\n","        # hu, hv: (batch, in_dim)\n","        h = torch.cat([hu, hv], dim=1)\n","        h = F.relu(self.lin(h))\n","        return self.out(h).squeeze(-1)\n","\n","# instantiate\n","in_dim = data.num_node_features\n","encoder = GATEncoder(in_dim=in_dim, hidden_dim=256, n_heads=4, n_layers=2, dropout=0.2).to(device)\n","predictor = LinkPredictor(encoder.out_dim).to(device)\n","\n","optimizer = Adam(list(encoder.parameters()) + list(predictor.parameters()), lr=1e-3, weight_decay=1e-5)\n","criterion = nn.BCEWithLogitsLoss()\n"],"metadata":{"id":"DyQ-NXbsO271"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training loop (mini-batch approach)"],"metadata":{"id":"ExG6hjxAIUUL"}},{"cell_type":"code","source":["data = data.to(device)\n","x_all = data.x\n","edge_index_all = data.edge_index\n","\n","# onvert training pairs to device tensors\n","pos_u, pos_v = pos_u.to(device), pos_v.to(device)\n","neg_u, neg_v = neg_u.to(device), neg_v.to(device)\n","\n","# Combine into a dataset for shuffled training\n","num_epochs = 30\n","batch_size = 256\n","pos_pairs = list(zip(pos_u.tolist(), pos_v.tolist()))\n","neg_pairs = list(zip(neg_u.tolist(), neg_v.tolist()))\n","labels_pos = [1]*len(pos_pairs)\n","labels_neg = [0]*len(neg_pairs)\n","train_pairs = pos_pairs + neg_pairs\n","train_labels = labels_pos + labels_neg\n","\n","# shuffle\n","perm = list(range(len(train_pairs)))\n","random.shuffle(perm)\n","train_pairs = [train_pairs[i] for i in perm]\n","train_labels = [train_labels[i] for i in perm]\n","\n","def batchify(pairs_batch):\n","    u = torch.tensor([p[0] for p in pairs_batch], dtype=torch.long, device=device)\n","    v = torch.tensor([p[1] for p in pairs_batch], dtype=torch.long, device=device)\n","    return u, v\n","\n","for epoch in range(1, num_epochs+1):\n","    encoder.train(); predictor.train()\n","    total_loss = 0.0\n","    for i in range(0, len(train_pairs), batch_size):\n","        batch_pairs = train_pairs[i:i+batch_size]\n","        batch_labels = torch.tensor(train_labels[i:i+batch_size], dtype=torch.float, device=device)\n","\n","        optimizer.zero_grad()\n","        node_reps = encoder(x_all, edge_index_all)  # (N, D)\n","        u_idx, v_idx = batchify(batch_pairs)\n","        hu = node_reps[u_idx]\n","        hv = node_reps[v_idx]\n","        logits = predictor(hu, hv)  # (batch,)\n","        loss = criterion(logits, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * len(batch_pairs)\n","\n","    avg_loss = total_loss / len(train_pairs)\n","    # simple evaluation: compute recall on validation (reuse training positives for demo)\n","    encoder.eval(); predictor.eval()\n","    with torch.no_grad():\n","        node_reps = encoder(x_all, edge_index_all)\n","        # score all candidate pairs in pos_edges for retrieval (simple)\n","        pos_u_t = torch.tensor([p[0] for p in pos_pairs], device=device)\n","        pos_v_t = torch.tensor([p[1] for p in pos_pairs], device=device)\n","        pos_scores = predictor(node_reps[pos_u_t], node_reps[pos_v_t]).sigmoid().cpu().numpy()\n","        avg_pos_score = pos_scores.mean()\n","\n","        neg_u_t = torch.tensor([p[0] for p in neg_pairs], device=device)\n","        neg_v_t = torch.tensor([p[1] for p in neg_pairs], device=device)\n","        neg_scores = predictor(node_reps[neg_u_t], node_reps[neg_v_t]).sigmoid().cpu().numpy()\n","        avg_neg_score = neg_scores.mean()\n","\n","    print(f\"Epoch {epoch:02d} | Loss: {avg_loss:.4f} | avg_pos_score: {avg_pos_score:.3f} | avg_neg_score: {avg_neg_score:.3f}\")\n"],"metadata":{"id":"YWwCYOBzO9ZT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Used trained model to predict missing alignment edges (link prediction) and add high-confidence edges..."],"metadata":{"id":"8PGKDG1MI2U-"}},{"cell_type":"code","source":["encoder.eval(); predictor.eval()\n","with torch.no_grad():\n","    node_reps = encoder(x_all, edge_index_all)\n","\n","# Candidate generation strategy:\n","# for each token in English, score all tokens in other languages within same sentence and add top-K above threshold.\n","predicted_edges = set()\n","score_threshold = 0.7\n","top_k = 2\n","\n","for s_idx, mapping in enumerate(sent_token_index):\n","    en_ids = mapping[\"en\"]\n","    for en_id in en_ids:\n","        # considered tokens in other langs within same sentence\n","        cand_ids = mapping[\"de\"] + mapping[\"es\"] + mapping[\"el\"]\n","        if not cand_ids:\n","            continue\n","        en_rep = node_reps[en_id].unsqueeze(0).repeat(len(cand_ids),1)\n","        cand_reps = node_reps[torch.tensor(cand_ids, device=device)]\n","        scores = predictor(en_rep, cand_reps).sigmoid().cpu().detach().numpy()\n","        # pick top_k\n","        top_idxs = np.argsort(scores)[-top_k:][::-1]\n","        for idx in top_idxs:\n","            score = float(scores[idx])\n","            if score >= score_threshold:\n","                u,v = sorted((en_id, cand_ids[idx]))\n","                predicted_edges.add((u,v,score))\n","\n","print(\"Predicted new edges (count):\", len(predicted_edges))\n","# show a few with token text\n","for u,v,score in list(predicted_edges)[:20]:\n","    print(f\"{u}({G.nodes[u]['token']},{G.nodes[u]['lang']}) <-> {v}({G.nodes[v]['token']},{G.nodes[v]['lang']})  score={score:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWH0PW0HPASs","executionInfo":{"status":"ok","timestamp":1763994140987,"user_tz":-120,"elapsed":344,"user":{"displayName":"MANQOBA Nkosi","userId":"08876572203614423140"}},"outputId":"a23ac9f6-8cbb-416b-b226-63d71d7121c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted new edges (count): 1536\n","1106(cat,en) <-> 1121(γάτα,el)  score=0.970\n","1553(dog,en) <-> 1558(Hund,de)  score=1.000\n","2460(cat,en) <-> 2469(γάτα,el)  score=1.000\n","1827(The,en) <-> 1834(El,es)  score=0.999\n","1437(the,en) <-> 1449(Η,el)  score=0.982\n","1611(blue,en) <-> 1613(ist,de)  score=1.000\n","1014(see,en) <-> 1017(sehen,de)  score=0.977\n","1914(love,en) <-> 1917(Ich,de)  score=0.989\n","355(The,en) <-> 362(El,es)  score=0.995\n","1313(the,en) <-> 1316(Frau,de)  score=1.000\n","662(is,en) <-> 670(azul,es)  score=0.999\n","2443(see,en) <-> 2456(το,el)  score=0.998\n","2965(big,en) <-> 2971(grande,es)  score=0.999\n","723(dog,en) <-> 731(es,es)  score=0.999\n","3067(the,en) <-> 3071(die,de)  score=1.000\n","2471(small,en) <-> 2481(Η,el)  score=1.000\n","2715(the,en) <-> 2728(γυναίκα,el)  score=1.000\n","1332(and,en) <-> 1345(Η,el)  score=0.999\n","1667(A,en) <-> 1674(grande,es)  score=0.994\n","2979(love,en) <-> 2984(lieben,de)  score=1.000\n"]}]},{"cell_type":"markdown","source":["Evaluate recall/precision on withheld gold alignments"],"metadata":{"id":"DcUfIHQ2J4F4"}},{"cell_type":"code","source":["gold = set(initial_alignment_edges)\n","pred_set = set(tuple(sorted((u,v))) for u,v,_ in predicted_edges)\n","\n","tp = len(gold & pred_set)\n","precision = tp / len(pred_set) if pred_set else 0.0\n","recall = tp / len(gold) if gold else 0.0\n","print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, TP: {tp}, Pred: {len(pred_set)}, Gold: {len(gold)}\")\n","# trash!!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzyihZlyPgxY","executionInfo":{"status":"ok","timestamp":1763994189516,"user_tz":-120,"elapsed":47,"user":{"displayName":"MANQOBA Nkosi","userId":"08876572203614423140"}},"outputId":"ec946af4-428e-4dec-b1a6-a9f993f8770c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.256, Recall: 0.224, TP: 393, Pred: 1536, Gold: 1757\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"I1AfgzmzPzxy"},"execution_count":null,"outputs":[]}]}